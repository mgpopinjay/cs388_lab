{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "extract_features() missing 1 required positional argument: 'sentence'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c8572c081f17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;31m#print(perceptron.weight_vector)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperceptron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_corpus_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-c8572c081f17>\u001b[0m in \u001b[0;36mcreate_corpus_vocab\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;31m# compute sparse feature vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0msentence_feature_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeat_extractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_exs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;31m# update corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: extract_features() missing 1 required positional argument: 'sentence'"
     ]
    }
   ],
   "source": [
    "# models.py\n",
    "\n",
    "from sentiment_data import *\n",
    "from utils import *\n",
    "\n",
    "from collections import Counter\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "class FeatureExtractor(object):\n",
    "    \"\"\"\n",
    "    Feature extraction base type. Takes a sentence and returns an indexed list of features.\n",
    "    \"\"\"\n",
    "    def get_indexer(self):\n",
    "        raise Exception(\"Don't call me, call my subclasses\")\n",
    "\n",
    "    def extract_features(self, sentence: List[str], add_to_indexer: bool=False) -> Counter:\n",
    "        \"\"\"\n",
    "        Extract features from a sentence represented as a list of words. Includes a flag add_to_indexer to\n",
    "        :param sentence: words in the example to featurize\n",
    "        :param add_to_indexer: True if we should grow the dimensionality of the featurizer if new features are encountered.\n",
    "        At test time, any unseen features should be discarded, but at train time, we probably want to keep growing it.\n",
    "        :return: A feature vector. We suggest using a Counter[int], which can encode a sparse feature vector (only\n",
    "        a few indices have nonzero value) in essentially the same way as a map. However, you can use whatever data\n",
    "        structure you prefer, since this does not interact with the framework code.\n",
    "        \"\"\"\n",
    "        raise Exception(\"Don't call me, call my subclasses\")\n",
    "\n",
    "\n",
    "class UnigramFeatureExtractor(FeatureExtractor): # 1.29 TODO: make sure works with data structure (SentimentExample object)\n",
    "    \"\"\"\n",
    "    Extracts unigram bag-of-words features from a sentence. It's up to you to decide how you want to handle counts\n",
    "    and any additional preprocessing you want to do.\n",
    "    \"\"\"\n",
    "    def __init__(self, indexer: Indexer):\n",
    "        self.indexer = Indexer\n",
    "        \n",
    "        #raise Exception(\"Must be implemented\")\n",
    "\n",
    "    def extract_features(self, sentence: List[str], add_to_indexer: bool=False) -> Counter:\n",
    "        \"\"\"\n",
    "        Extracts unigram bag-of-words features from a sentence reflecting feature counts. \n",
    "        The sentence preprocessing involves: lower casing, punctutation removal, contraction expansion\n",
    "        :param sentence: words in the example to featurize.\n",
    "        :param add_to_indexer: True if we should grow the dimensionality of the featurizer if new features are encountered.\n",
    "        :return: A feature vector.\n",
    "        \"\"\"        \n",
    "        # import string # punctuation removal\n",
    "        \n",
    "        global features\n",
    "        \n",
    "        punctuations = string.punctuation\n",
    "        sentence = [word.lower() for word in sentence if word not in punctuations] \n",
    "        features = Counter(sentence)\n",
    "        return features\n",
    "    \n",
    "        \n",
    "    def feature_vector_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Get the size of the feature vector.\n",
    "        :return: vector size.\n",
    "        \"\"\"\n",
    "        return len(features)\n",
    "        \n",
    "    # data of data.txt is: [<class '__main__.SentimentExample'>, <class '__main__.SentimentExample'>, ...]\n",
    "    # (data[0].words)        \n",
    "\n",
    "class BigramFeatureExtractor(FeatureExtractor):\n",
    "    \"\"\"\n",
    "    Bigram feature extractor analogous to the unigram one.\n",
    "    \"\"\"\n",
    "    def __init__(self, indexer: Indexer):\n",
    "        raise Exception(\"Must be implemented\")\n",
    "\n",
    "\n",
    "class BetterFeatureExtractor(FeatureExtractor):\n",
    "    \"\"\"\n",
    "    Better feature extractor...try whatever you can think of!\n",
    "    \"\"\"\n",
    "    def __init__(self, indexer: Indexer):\n",
    "        raise Exception(\"Must be implemented\")\n",
    "\n",
    "\n",
    "class SentimentClassifier(object):\n",
    "    \"\"\"\n",
    "    Sentiment classifier base type\n",
    "    \"\"\"\n",
    "    def predict(self, sentence: List[str]) -> int:\n",
    "        \"\"\"\n",
    "        :param sentence: words (List[str]) in the sentence to classify\n",
    "        :return: Either 0 for negative class or 1 for positive class\n",
    "        \"\"\"\n",
    "        raise Exception(\"Don't call me, call my subclasses\")\n",
    "\n",
    "\n",
    "class TrivialSentimentClassifier(SentimentClassifier):\n",
    "    \"\"\"\n",
    "    Sentiment classifier that always predicts the positive class.\n",
    "    \"\"\"\n",
    "    def predict(self, sentence: List[str]) -> int:\n",
    "        return 1\n",
    "\n",
    "\n",
    "class PerceptronClassifier(SentimentClassifier):\n",
    "    \"\"\"\n",
    "    Implement this class -- you should at least have init() and implement the predict method from the SentimentClassifier\n",
    "    superclass. Hint: you'll probably need this class to wrap both the weight vector and featurizer -- feel free to\n",
    "    modify the constructor to pass these in.\n",
    "    \"\"\"\n",
    "    \n",
    "    #raise Exception(\"Must be implemented\")\n",
    "    \n",
    "    def __init__(self, feat_extractor: FeatureExtractor, train_exs: List[SentimentExample]):\n",
    "        self.feat_extractor = feat_extractor\n",
    "        self.train_exs = train_exs\n",
    "        \n",
    "        self.corpus_vocab = Counter()\n",
    "        self.weight_vector = Counter()\n",
    "        \n",
    "    def create_corpus_vocab(self) -> Counter: #TODO: ??cache??\n",
    "        \"\"\"\n",
    "        Creates the corpus vocabulary by aggregating sparse vectors of each sample (i.e. sentence) \n",
    "            in the training dataset.\n",
    "        :return: A Counter of all the words in the corpus and their frequencies.\n",
    "        \"\"\"         \n",
    "        \n",
    "        data_size = len(self.train_exs)\n",
    "        \n",
    "        # for each example in training data\n",
    "        for i in range(data_size):\n",
    "            \n",
    "            # compute sparse feature vector \n",
    "            sentence_feature_vector = self.feat_extractor.extract_features(self.train_exs[i])\n",
    "            \n",
    "            # update corpus \n",
    "            self.corpus_vocab += sentence_feature_vector\n",
    "            \n",
    "        return self.corpus_vocab\n",
    "\n",
    "                  \n",
    "    \n",
    "    def initialize_weight_vector(self) -> Counter: # should be the same size as the corpus vocab\n",
    "        \"\"\"\n",
    "        Initializes the weight vector (same size as the corpus vocabulary) with zeroes.\n",
    "        :return: A Counter of words with zeroes as weights.\n",
    "        \"\"\"           \n",
    "\n",
    "        for word in list(self.corpus_vocab): \n",
    "            self.weight_vector.update({word: 0})\n",
    "        \n",
    "        return self.weight_vector\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def dot_product(corpus_weight_Counter: Counter, sentence_feature_Counter: Counter) -> float: \n",
    "        \"\"\"\n",
    "        Computes the dot product of the weight vector and the feature vector for one sentence. \n",
    "        :param corpus_weight_Counter: vector of all corpus words and their current weights\n",
    "        :param sentence_feature_Counter: vector of sentence features and feature frequencies\n",
    "        :return: dot product value\n",
    "        \"\"\"   \n",
    "        \n",
    "        result = 0\n",
    "        for word in list(sentence_feature_Counter):\n",
    "            result += corpus_weight_Counter[word]\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def predicted_label(self, sentence_feature_Counter: Counter) -> bool: #?? needs self??\n",
    "        \"\"\"\n",
    "        Predicts a sentiment label for a single sample (i.e. sentence).\n",
    "        :param sentence_feature_Counter: vector of sentence features and feature frequencies\n",
    "        :return: label \n",
    "        \"\"\"     \n",
    "        \n",
    "        if dot_product(self.weight_vector, sentence_feature_vector) > 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0 \n",
    "\n",
    "\n",
    "    def update_weights(self, sentence_feature_Counter, operation = \"add\"):\n",
    "        \"\"\"\n",
    "        Updates weight vector using feature vector values.\n",
    "        :param X: \n",
    "        :return:  \n",
    "        \"\"\"         \n",
    "\n",
    "        for word in list(sentence_feature_Counter):\n",
    "            if operation == \"add\":\n",
    "                self.weight_vector[word] += 1\n",
    "            else:\n",
    "                self.weight_vector[word] -= 1\n",
    "        \n",
    "        \n",
    "    def train_classifier(self, epochs = 10):\n",
    "        # ?? need to compute loss anywhere??\n",
    "        \n",
    "        weight_vector = self.weight_vector.initialize_weight_vector() # ?? not sure this is right\n",
    "        \n",
    "        for i in range(epochs): \n",
    "            for j in range(len(self.train_exs)): # >> shuffle data\n",
    "                feature_vector = self.feat_extractor.extract_features(train_exs[j])\n",
    "                predicted_label = predicted_label(feature_vector)\n",
    "                \n",
    "                # TODO: learning rate??\n",
    "                # pred 0, true label = 1\n",
    "                if predicted_label < train_exs[j].label:  # ??? pay attention to data structure format here<<<\n",
    "                    #weight_vector += feature_vector ## TODO: fix feature vector\n",
    "                    update_weights(feature_vector, operation = \"add\")\n",
    "                    \n",
    "                \n",
    "                # pred 1, true label = 0\n",
    "                elif predicted_label > train_exs[j].label:  # ??? pay attention to data structure format here<<<\n",
    "                    #weight_vector -= feature_vector ## TODO: fix feature vector\n",
    "                    update_weights(feature_vector, operation = \"subtract\")\n",
    "            \n",
    "            # ?? no need to uodate self.weight_vector since the update_weights does it automatically???\n",
    "            #self.weight_vector =  weight_vector  # ?? not sure this is right\n",
    "            \n",
    "            \n",
    "        # return anything here??\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        # Make a Perceptron Classifier Model:\n",
    "        \n",
    "            #DONE 1 convert and example into features DONE\n",
    "            #DONE 2 initialize weight vector\n",
    "            \n",
    "            #3 run epochs\n",
    "            #4 compute label Done\n",
    "            #5 compute loss \n",
    "            #6 update weight vector\n",
    "                # repeat 3-6\n",
    "  \n",
    "\n",
    " \n",
    "                \n",
    "class LogisticRegressionClassifier(SentimentClassifier):\n",
    "    \"\"\"\n",
    "    Implement this class -- you should at least have init() and implement the predict method from the SentimentClassifier\n",
    "    superclass. Hint: you'll probably need this class to wrap both the weight vector and featurizer -- feel free to\n",
    "    modify the constructor to pass these in.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        raise Exception(\"Must be implemented\")\n",
    "\n",
    "\n",
    "def train_perceptron(train_exs: List[SentimentExample], feat_extractor: FeatureExtractor) -> PerceptronClassifier:\n",
    "    \"\"\"\n",
    "    Train a classifier with the perceptron.\n",
    "    :param train_exs: training set, List of SentimentExample objects\n",
    "    :param feat_extractor: feature extractor to use\n",
    "    :return: trained PerceptronClassifier model\n",
    "    \"\"\"\n",
    "    raise Exception(\"Must be implemented\")\n",
    "\n",
    "\n",
    "def train_logistic_regression(train_exs: List[SentimentExample], feat_extractor: FeatureExtractor) -> LogisticRegressionClassifier:\n",
    "    \"\"\"\n",
    "    Train a logistic regression model.\n",
    "    :param train_exs: training set, List of SentimentExample objects\n",
    "    :param feat_extractor: feature extractor to use\n",
    "    :return: trained LogisticRegressionClassifier model\n",
    "    \"\"\"\n",
    "    raise Exception(\"Must be implemented\")\n",
    "\n",
    "\n",
    "def train_model(args, train_exs: List[SentimentExample], dev_exs: List[SentimentExample]) -> SentimentClassifier:\n",
    "    \"\"\"\n",
    "    Main entry point for your modifications. Trains and returns one of several models depending on the args\n",
    "    passed in from the main method. You may modify this function, but probably will not need to.\n",
    "    :param args: args bundle from sentiment_classifier.py\n",
    "    :param train_exs: training set, List of SentimentExample objects\n",
    "    :param dev_exs: dev set, List of SentimentExample objects. You can use this for validation throughout the training\n",
    "    process, but you should *not* directly train on this data.\n",
    "    :return: trained SentimentClassifier model, of whichever type is specified\n",
    "    \"\"\"\n",
    "    # Initialize feature extractor\n",
    "    if args.model == \"TRIVIAL\":\n",
    "        feat_extractor = None\n",
    "    elif args.feats == \"UNIGRAM\":\n",
    "        # Add additional preprocessing code here\n",
    "        feat_extractor = UnigramFeatureExtractor(Indexer())\n",
    "    elif args.feats == \"BIGRAM\":\n",
    "        # Add additional preprocessing code here\n",
    "        feat_extractor = BigramFeatureExtractor(Indexer())\n",
    "    elif args.feats == \"BETTER\":\n",
    "        # Add additional preprocessing code here\n",
    "        feat_extractor = BetterFeatureExtractor(Indexer())\n",
    "    else:\n",
    "        raise Exception(\"Pass in UNIGRAM, BIGRAM, or BETTER to run the appropriate system\")\n",
    "\n",
    "    # Train the model\n",
    "    if args.model == \"TRIVIAL\":\n",
    "        model = TrivialSentimentClassifier()\n",
    "    elif args.model == \"PERCEPTRON\":\n",
    "        model = train_perceptron(train_exs, feat_extractor)\n",
    "    elif args.model == \"LR\":\n",
    "        model = train_logistic_regression(train_exs, feat_extractor)\n",
    "    else:\n",
    "        raise Exception(\"Pass in TRIVIAL, PERCEPTRON, or LR to run the appropriate system\")\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###############################################------ Unit Tests ------################################################\n",
    "    \n",
    "### UnigramFeatureExtractor ###\n",
    "#unigram_extractor = UnigramFeatureExtractor(Indexer)\n",
    "#print(unigram_extractor) \n",
    "#print(unigram_extractor.extract_features([\"I\", \"am\", \"here\", \".\", \"I\", \"made\", \"this\", \",\", \"Class\"]))\n",
    "#print(unigram_extractor.feature_vector_size()) \n",
    "    \n",
    "### PerceptronClassifier ###\n",
    "#train_exs = read_blind_sst_examples(\"data/train.txt\")[0]\n",
    "    \n",
    "train_exs = [\"I\", \"am\", \"here\", \".\", \"I\", \"made\", \"this\", \",\", \"Class\"]\n",
    "perceptron = PerceptronClassifier(UnigramFeatureExtractor, train_exs)  \n",
    "#print(type(perceptron))  \n",
    "#print(perceptron.feat_extractor)\n",
    "#print(perceptron.train_exs) # prints first sentence of the dataset\n",
    "#print(perceptron.corpus_vocab)\n",
    "#print(perceptron.weight_vector)\n",
    "\n",
    "print(perceptron.create_corpus_vocab())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#testing = FeatureExtractor()\n",
    "#testing.echo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
